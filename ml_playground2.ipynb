{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7938c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing - Feature Selection and Target Variable\n",
    "# Selected Features\n",
    "selected_features = [\n",
    "    'study_year',\n",
    "    'assignment_delay_frequency',\n",
    "    'procrastination_reasons',\n",
    "    'last_minute_exam_preparation',\n",
    "    'study_hours_per_week',\n",
    "    'use_of_time_management',\n",
    "    'procrastination_management_training',\n",
    "    'procrastination_recovery_strategies',\n",
    "    'hours_spent_on_mobile_non_academic',\n",
    "    'study_session_distractions'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target_variable = 'procrastination_and_grade_outcome'\n",
    "\n",
    "# Check if the target variable exists in the DataFrame\n",
    "if target_variable not in df.columns:\n",
    "    print(f\"Error: The target variable '{target_variable}' is not in the DataFrame.\")\n",
    "else:\n",
    "    print(f\"Target variable: {target_variable}\")\n",
    "    # Display distribution of the target variable\n",
    "    print(\"\\nDistribution of the target variable:\")\n",
    "    print(df[target_variable].value_counts())\n",
    "    \n",
    "    # Binary encoding of the target variable (if needed)\n",
    "    if df[target_variable].dtype == 'object':\n",
    "        print(\"\\nUnique values of the target variable:\", df[target_variable].unique())\n",
    "        \n",
    "        # If the target variable is 'Yes' and 'No', convert to 1 and 0\n",
    "        if set(df[target_variable].unique()) == {'Yes', 'No'}:\n",
    "            df['target'] = (df[target_variable] == 'Yes').astype(int)\n",
    "            print(\"Target variable converted to binary format (Yes=1, No=0)\")\n",
    "        else:\n",
    "            # For other values use a LabelEncoder\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            df['target'] = le.fit_transform(df[target_variable])\n",
    "            print(\"Target variable encoded with LabelEncoder:\")\n",
    "            for i, label in enumerate(le.classes_):\n",
    "                print(f\"  {label} -> {i}\")\n",
    "    else:\n",
    "        # If already numeric, use directly\n",
    "        df['target'] = df[target_variable]\n",
    "\n",
    "# Select features and encoded target variable\n",
    "X = df[selected_features]\n",
    "y = df['target']\n",
    "\n",
    "print(f\"\\nSelected Features for the model ({len(selected_features)}):\")\n",
    "for feature in selected_features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"\\nCategorical columns:\", categorical_cols)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining data: {X_train.shape[0]} samples\")\n",
    "print(f\"Test data: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Data preprocessing - Pipeline for categorical variables\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Pipeline for categorical variables: Imputation and One-Hot-Encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Pipeline for numerical variables (if any)\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        ('num', numerical_transformer, numerical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model Training and Evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create and train the Logistic Regression model with Grid Search\n",
    "print(\"Training the Logistic Regression model with Grid Search...\")\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "lr_param_grid = {\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['l2'],\n",
    "    'classifier__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "lr_grid_search = GridSearchCV(\n",
    "    lr_pipeline, \n",
    "    lr_param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"\\nBest parameters for Logistic Regression:\")\n",
    "print(lr_grid_search.best_params_)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred_lr = lr_grid_search.predict(X_test)\n",
    "print(\"\\nLogistic Regression - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Create and plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot ROC curve\n",
    "if len(np.unique(y_test)) == 2:  # Only for binary classification\n",
    "    y_pred_proba = lr_grid_search.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve - Logistic Regression')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Train and evaluate Random Forest model\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_pipeline, \n",
    "    rf_param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"\\nBest parameters for Random Forest:\")\n",
    "print(rf_grid_search.best_params_)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred_rf = rf_grid_search.predict(X_test)\n",
    "print(\"\\nRandom Forest - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Create and plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot ROC curve\n",
    "if len(np.unique(y_test)) == 2:  # Only for binary classification\n",
    "    y_pred_proba_rf = rf_grid_search.predict_proba(X_test)[:, 1]\n",
    "    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
    "    auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc:.2f})')\n",
    "    plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve Comparison')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Feature importance analysis for Random Forest\n",
    "if hasattr(rf_grid_search.best_estimator_.named_steps['classifier'], 'feature_importances_'):\n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = []\n",
    "    \n",
    "    # Get the column transformer\n",
    "    ct = rf_grid_search.best_estimator_.named_steps['preprocessor']\n",
    "    \n",
    "    # Get all transformers\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name == 'cat':\n",
    "            # For categorical features, get the one-hot encoded feature names\n",
    "            for i, col in enumerate(cols):\n",
    "                feature_names.extend([f\"{col}_{val}\" for val in trans.named_steps['onehot'].get_feature_names_out([col])])\n",
    "        else:\n",
    "            # For numerical features, use the column names\n",
    "            feature_names.extend(cols)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = rf_grid_search.best_estimator_.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for visualization\n",
    "    if len(feature_names) == len(importances):\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot feature importances\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(20))\n",
    "        plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(feature_importance_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
